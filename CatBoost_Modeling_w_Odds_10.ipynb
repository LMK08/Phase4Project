{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7de8a8-4eec-4a16-98db-a0dad0554923",
   "metadata": {},
   "source": [
    "## CatBoost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5f3bb-0d31-4c60-9be9-a860b23150a9",
   "metadata": {},
   "source": [
    "We have established CatBoost as the top performer among our classification models on the numerical data. It has the additional benefit of being able to accept categorical features without one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23218ba2-9574-4615-95d4-203780983536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9913cc4e-e6bf-473b-a3a2-a9df158af341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EPL_Updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b47ace-ba57-46ac-9a38-5ea1b45c3bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pinnacle Closing Home Win Odds', 'Pinnacle Closing Draw Odds',\n",
       "       'Pinnacle Closing Away Win Odds', 'date', 'home_team', 'away_team',\n",
       "       'week', 'date.1', 'home_team.1', 'home_xg', 'score', 'away_xg',\n",
       "       'away_team.1', 'referee', 'game_id', 'home_team_elo', 'away_team_elo',\n",
       "       'season', 'home_xG_to_date', 'away_xG_to_date',\n",
       "       'home_xG_against_to_date', 'away_xG_against_to_date',\n",
       "       'home_goals_scored', 'away_goals_scored', 'home_goals_scored_to_date',\n",
       "       'away_goals_scored_to_date', 'home_goals_conceded_to_date',\n",
       "       'away_goals_conceded_to_date', 'home_match_points', 'away_match_points',\n",
       "       'home_points_to_date', 'away_points_to_date', 'home_form', 'away_form',\n",
       "       'match_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f384afe-dfd8-4d9f-ba4a-9e0e471c18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2660 entries, 0 to 2659\n",
      "Data columns (total 35 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Pinnacle Closing Home Win Odds  2660 non-null   float64\n",
      " 1   Pinnacle Closing Draw Odds      2660 non-null   float64\n",
      " 2   Pinnacle Closing Away Win Odds  2660 non-null   float64\n",
      " 3   date                            2660 non-null   object \n",
      " 4   home_team                       2660 non-null   object \n",
      " 5   away_team                       2660 non-null   object \n",
      " 6   week                            2280 non-null   float64\n",
      " 7   date.1                          2280 non-null   object \n",
      " 8   home_team.1                     2280 non-null   object \n",
      " 9   home_xg                         2280 non-null   float64\n",
      " 10  score                           2280 non-null   object \n",
      " 11  away_xg                         2280 non-null   float64\n",
      " 12  away_team.1                     2280 non-null   object \n",
      " 13  referee                         2280 non-null   object \n",
      " 14  game_id                         2280 non-null   object \n",
      " 15  home_team_elo                   2280 non-null   float64\n",
      " 16  away_team_elo                   2280 non-null   float64\n",
      " 17  season                          2280 non-null   float64\n",
      " 18  home_xG_to_date                 2280 non-null   float64\n",
      " 19  away_xG_to_date                 2280 non-null   float64\n",
      " 20  home_xG_against_to_date         2280 non-null   float64\n",
      " 21  away_xG_against_to_date         2280 non-null   float64\n",
      " 22  home_goals_scored               2280 non-null   float64\n",
      " 23  away_goals_scored               2280 non-null   float64\n",
      " 24  home_goals_scored_to_date       2280 non-null   float64\n",
      " 25  away_goals_scored_to_date       2280 non-null   float64\n",
      " 26  home_goals_conceded_to_date     2280 non-null   float64\n",
      " 27  away_goals_conceded_to_date     2280 non-null   float64\n",
      " 28  home_match_points               2280 non-null   float64\n",
      " 29  away_match_points               2280 non-null   float64\n",
      " 30  home_points_to_date             2280 non-null   float64\n",
      " 31  away_points_to_date             2280 non-null   float64\n",
      " 32  home_form                       2280 non-null   float64\n",
      " 33  away_form                       2280 non-null   float64\n",
      " 34  match_result                    2280 non-null   float64\n",
      "dtypes: float64(26), object(9)\n",
      "memory usage: 727.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7904b525-4d64-4841-8685-b89cb4fc06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['week', 'date','home_xg', 'score', 'away_xg','game_id',\n",
    "                   'season','home_match_points', 'away_match_points', \n",
    "                   'home_goals_scored', 'away_goals_scored', 'date.1', 'home_team.1', 'away_team.1', 'referee', 'home_team',\n",
    " 'away_team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457e3771-22a4-45df-8150-b87524293702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pinnacle Closing Home Win Odds', 'Pinnacle Closing Draw Odds',\n",
       "       'Pinnacle Closing Away Win Odds', 'home_team_elo', 'away_team_elo',\n",
       "       'home_xG_to_date', 'away_xG_to_date', 'home_xG_against_to_date',\n",
       "       'away_xG_against_to_date', 'home_goals_scored_to_date',\n",
       "       'away_goals_scored_to_date', 'home_goals_conceded_to_date',\n",
       "       'away_goals_conceded_to_date', 'home_points_to_date',\n",
       "       'away_points_to_date', 'home_form', 'away_form', 'match_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the first few rows of the dataframe after dropping the columns\n",
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874479f7-7abb-4a5a-96e8-f54c9bd8b47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pinnacle Closing Home Win Odds',\n",
       " 'Pinnacle Closing Draw Odds',\n",
       " 'Pinnacle Closing Away Win Odds',\n",
       " 'home_team_elo',\n",
       " 'away_team_elo',\n",
       " 'home_xG_to_date',\n",
       " 'away_xG_to_date',\n",
       " 'home_xG_against_to_date',\n",
       " 'away_xG_against_to_date',\n",
       " 'home_goals_scored_to_date',\n",
       " 'away_goals_scored_to_date',\n",
       " 'home_goals_conceded_to_date',\n",
       " 'away_goals_conceded_to_date',\n",
       " 'home_points_to_date',\n",
       " 'away_points_to_date',\n",
       " 'home_form',\n",
       " 'away_form',\n",
       " 'match_result']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_order = df_model.columns.tolist()\n",
    "feature_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef287f8-261a-42f6-a39d-ae2b43c37171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pinnacle Closing Home Win Odds</th>\n",
       "      <th>Pinnacle Closing Draw Odds</th>\n",
       "      <th>Pinnacle Closing Away Win Odds</th>\n",
       "      <th>home_team_elo</th>\n",
       "      <th>away_team_elo</th>\n",
       "      <th>home_xG_to_date</th>\n",
       "      <th>away_xG_to_date</th>\n",
       "      <th>home_xG_against_to_date</th>\n",
       "      <th>away_xG_against_to_date</th>\n",
       "      <th>home_goals_scored_to_date</th>\n",
       "      <th>away_goals_scored_to_date</th>\n",
       "      <th>home_goals_conceded_to_date</th>\n",
       "      <th>away_goals_conceded_to_date</th>\n",
       "      <th>home_points_to_date</th>\n",
       "      <th>away_points_to_date</th>\n",
       "      <th>home_form</th>\n",
       "      <th>away_form</th>\n",
       "      <th>match_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.49</td>\n",
       "      <td>4.73</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1884.934448</td>\n",
       "      <td>1697.498169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.75</td>\n",
       "      <td>6.15</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1673.780518</td>\n",
       "      <td>1576.490356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.40</td>\n",
       "      <td>12.25</td>\n",
       "      <td>1633.799683</td>\n",
       "      <td>1692.951660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.79</td>\n",
       "      <td>3.56</td>\n",
       "      <td>5.51</td>\n",
       "      <td>1567.101318</td>\n",
       "      <td>1837.004272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.82</td>\n",
       "      <td>3.49</td>\n",
       "      <td>5.42</td>\n",
       "      <td>1670.871338</td>\n",
       "      <td>1914.848877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>1.63</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>1.13</td>\n",
       "      <td>10.69</td>\n",
       "      <td>16.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>3.11</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>1.07</td>\n",
       "      <td>15.01</td>\n",
       "      <td>27.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>7.99</td>\n",
       "      <td>6.11</td>\n",
       "      <td>1.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pinnacle Closing Home Win Odds  Pinnacle Closing Draw Odds  \\\n",
       "0                               1.49                        4.73   \n",
       "1                              11.75                        6.15   \n",
       "2                               1.33                        5.40   \n",
       "3                               1.79                        3.56   \n",
       "4                               1.82                        3.49   \n",
       "...                              ...                         ...   \n",
       "2655                            1.63                        4.64   \n",
       "2656                            1.13                       10.69   \n",
       "2657                            3.11                        3.99   \n",
       "2658                            1.07                       15.01   \n",
       "2659                            7.99                        6.11   \n",
       "\n",
       "      Pinnacle Closing Away Win Odds  home_team_elo  away_team_elo  \\\n",
       "0                               7.25    1884.934448    1697.498169   \n",
       "1                               1.29    1673.780518    1576.490356   \n",
       "2                              12.25    1633.799683    1692.951660   \n",
       "3                               5.51    1567.101318    1837.004272   \n",
       "4                               5.42    1670.871338    1914.848877   \n",
       "...                              ...            ...            ...   \n",
       "2655                            4.99            NaN            NaN   \n",
       "2656                           16.27            NaN            NaN   \n",
       "2657                            2.19            NaN            NaN   \n",
       "2658                           27.84            NaN            NaN   \n",
       "2659                            1.35            NaN            NaN   \n",
       "\n",
       "      home_xG_to_date  away_xG_to_date  home_xG_against_to_date  \\\n",
       "0                 0.0              0.0                      0.0   \n",
       "1                 0.0              0.0                      0.0   \n",
       "2                 0.0              0.0                      0.0   \n",
       "3                 0.0              0.0                      0.0   \n",
       "4                 0.0              0.0                      0.0   \n",
       "...               ...              ...                      ...   \n",
       "2655              NaN              NaN                      NaN   \n",
       "2656              NaN              NaN                      NaN   \n",
       "2657              NaN              NaN                      NaN   \n",
       "2658              NaN              NaN                      NaN   \n",
       "2659              NaN              NaN                      NaN   \n",
       "\n",
       "      away_xG_against_to_date  home_goals_scored_to_date  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         0.0                        0.0   \n",
       "...                       ...                        ...   \n",
       "2655                      NaN                        NaN   \n",
       "2656                      NaN                        NaN   \n",
       "2657                      NaN                        NaN   \n",
       "2658                      NaN                        NaN   \n",
       "2659                      NaN                        NaN   \n",
       "\n",
       "      away_goals_scored_to_date  home_goals_conceded_to_date  \\\n",
       "0                           0.0                          0.0   \n",
       "1                           0.0                          0.0   \n",
       "2                           0.0                          0.0   \n",
       "3                           0.0                          0.0   \n",
       "4                           0.0                          0.0   \n",
       "...                         ...                          ...   \n",
       "2655                        NaN                          NaN   \n",
       "2656                        NaN                          NaN   \n",
       "2657                        NaN                          NaN   \n",
       "2658                        NaN                          NaN   \n",
       "2659                        NaN                          NaN   \n",
       "\n",
       "      away_goals_conceded_to_date  home_points_to_date  away_points_to_date  \\\n",
       "0                             0.0                  0.0                  0.0   \n",
       "1                             0.0                  0.0                  0.0   \n",
       "2                             0.0                  0.0                  0.0   \n",
       "3                             0.0                  0.0                  0.0   \n",
       "4                             0.0                  0.0                  0.0   \n",
       "...                           ...                  ...                  ...   \n",
       "2655                          NaN                  NaN                  NaN   \n",
       "2656                          NaN                  NaN                  NaN   \n",
       "2657                          NaN                  NaN                  NaN   \n",
       "2658                          NaN                  NaN                  NaN   \n",
       "2659                          NaN                  NaN                  NaN   \n",
       "\n",
       "      home_form  away_form  match_result  \n",
       "0           0.0        0.0           0.0  \n",
       "1           0.0        0.0           0.0  \n",
       "2           0.0        0.0           2.0  \n",
       "3           0.0        0.0           2.0  \n",
       "4           0.0        0.0           2.0  \n",
       "...         ...        ...           ...  \n",
       "2655        NaN        NaN           NaN  \n",
       "2656        NaN        NaN           NaN  \n",
       "2657        NaN        NaN           NaN  \n",
       "2658        NaN        NaN           NaN  \n",
       "2659        NaN        NaN           NaN  \n",
       "\n",
       "[2660 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096aff71-3c32-42a2-a649-98950b31f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/lkimball/Desktop/Betting/TotalSet_Prob_distribution.csv'\n",
    "df_model.to_csv(csv_file_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb99ecf-0103-46b8-9a84-fe3ad22a2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0794822\ttotal: 60.2ms\tremaining: 5.96s\n",
      "10:\tlearn: 0.9910403\ttotal: 76.6ms\tremaining: 619ms\n",
      "20:\tlearn: 0.9645600\ttotal: 90.4ms\tremaining: 340ms\n",
      "30:\tlearn: 0.9508591\ttotal: 104ms\tremaining: 232ms\n",
      "40:\tlearn: 0.9413634\ttotal: 118ms\tremaining: 170ms\n",
      "50:\tlearn: 0.9335290\ttotal: 132ms\tremaining: 127ms\n",
      "60:\tlearn: 0.9284762\ttotal: 149ms\tremaining: 95ms\n",
      "70:\tlearn: 0.9223885\ttotal: 166ms\tremaining: 67.8ms\n",
      "80:\tlearn: 0.9174896\ttotal: 185ms\tremaining: 43.3ms\n",
      "90:\tlearn: 0.9105336\ttotal: 207ms\tremaining: 20.5ms\n",
      "99:\tlearn: 0.9051728\ttotal: 226ms\tremaining: 0us\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/jkmqh6017hb_3wwh71glzdrc0000gn/T/ipykernel_14858/4116522415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Calculate the Log Loss with the best estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mlogloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Best Log Loss: {logloss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2385\u001b[0m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2387\u001b[0;31m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"multioutput\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Assuming df_dropped is your DataFrame after dropping columns\n",
    "\n",
    "# Define the target variable and the feature set\n",
    "X = df_model.drop('match_result', axis=1)\n",
    "y = df_model['match_result']\n",
    "\n",
    "# Identify categorical features for CatBoost\n",
    "categorical_features_indices = list(X.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# Determine the number of records to include in the test set\n",
    "num_test_records = 380\n",
    "\n",
    "# Split the features data\n",
    "X_train = X.iloc[:-num_test_records]\n",
    "X_test = X.iloc[-num_test_records:]\n",
    "\n",
    "# Split the target data\n",
    "y_train = y.iloc[:-num_test_records]\n",
    "y_test = y.iloc[-num_test_records:]\n",
    "\n",
    "# Separate numerical features\n",
    "numerical_features = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the numerical features using only the training data\n",
    "X_train[numerical_features.columns] = scaler.fit_transform(X_train[numerical_features.columns])\n",
    "\n",
    "# Use the same scaling parameters to scale the numerical features in the testing data\n",
    "X_test[numerical_features.columns] = scaler.transform(X_test[numerical_features.columns])\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=3,\n",
    "    cat_features=categorical_features_indices,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Train CatBoostClassifier\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Predict probabilities on the testing set using the best estimator\n",
    "y_pred_proba = catboost_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate the Log Loss with the best estimator\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(f'Best Log Loss: {logloss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3497e-3a53-4b32-b56c-032686323762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9397247-a9a5-4d09-8ae9-03d20a67fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier instance\n",
    "catboost = CatBoostClassifier(cat_features=categorical_features_indices, verbose=0)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', catboost)\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid to be tested\n",
    "params_grid = {\n",
    "    'classifier__iterations': [25, 50, 100],\n",
    "    'classifier__depth': [5, 6, 7, 8],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "    # You can add other parameters you want to tune\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV object\n",
    "# Note: Scoring is now 'neg_log_loss' to use log loss. The \"negative\" is because GridSearchCV always tries to maximize its score, \n",
    "# so negating the log loss makes it a score to be maximized (lower log loss is better).\n",
    "grid_search = GridSearchCV(pipeline, params_grid, cv=5, scoring='neg_log_loss', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Predict probabilities on the testing set using the best estimator\n",
    "y_pred_proba = best_estimator.predict_proba(X_test)\n",
    "\n",
    "# Calculate the Log Loss with the best estimator\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best Log Loss: {logloss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960083cc-5b9b-43ee-b058-db9281e93609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the scaler object\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5093f1-6057-40ea-abc5-a7f25124ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the best estimator\n",
    "feature_importances = best_estimator.named_steps['classifier'].get_feature_importance()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame to show the most important features at the top\n",
    "importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "print(importances_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85286d7-18c5-4b1b-b6a8-362192d0bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure your DataFrame 'importances_df' is defined as in your provided code\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importances_df, x='Importance', y='Feature')\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73e5c3-d923-41b4-b26a-35fae7c311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test, and categorical_features_indices are already defined\n",
    "\n",
    "# Create a CatBoost classifier instance\n",
    "catboost = CatBoostClassifier(cat_features=categorical_features_indices, verbose=0)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Here you can add other preprocessing steps if needed\n",
    "    ('classifier', catboost)\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid to be tested\n",
    "params_grid = {\n",
    "    'classifier__iterations': [25, 50, 100],\n",
    "    'classifier__depth': [5, 6, 7, 8],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "    # You can add other parameters you want to tune\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV object\n",
    "# Change scoring to 'neg_log_loss'\n",
    "grid_search = GridSearchCV(pipeline, params_grid, cv=5, scoring='neg_log_loss', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the testing set using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Predict probabilities for log loss calculation\n",
    "y_pred_proba = best_estimator.predict_proba(X_test)\n",
    "\n",
    "# Calculate various scores with the best estimator\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Print the best parameters and all scores\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Log Loss: {logloss:.4f}')\n",
    "\n",
    "# Print the full classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f037d0-7acf-47ba-9f0e-0084aa946a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3ad80-e834-461c-8397-dad0da171340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test, and categorical_features_indices are already defined\n",
    "\n",
    "# Create a CatBoost classifier instance\n",
    "catboost = CatBoostClassifier(cat_features=categorical_features_indices, verbose=0)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Here you can add other preprocessing steps if needed\n",
    "    ('classifier', catboost)\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid to be tested\n",
    "params_grid = {\n",
    "    'classifier__iterations': [25, 50, 100],\n",
    "    'classifier__depth': [5, 6, 7, 8],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "    # You can add other parameters you want to tune\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, params_grid, cv=5, scoring='neg_log_loss', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the testing set using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Predict probabilities for log loss calculation\n",
    "y_pred_proba = best_estimator.predict_proba(X_test)\n",
    "\n",
    "# Calculate various scores with the best estimator\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Print the best parameters and all scores\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Log Loss: {logloss:.4f}')\n",
    "\n",
    "# Print the full classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "class_wise_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print class-wise accuracy\n",
    "print(\"\\nClass-wise Accuracy:\")\n",
    "for i, accuracy in enumerate(class_wise_accuracy):\n",
    "    print(f'Class {i} Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aea909-93a1-471e-befc-c7d7f7e4f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5c802-14df-4f45-90c9-c29c23c491f9",
   "metadata": {},
   "source": [
    "## K-Folds so I can plot log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b09023-8608-4f2d-b672-cb058a74d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a TimeSeriesSplit cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "best_log_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "# Define your hyperparameters grid to be tested\n",
    "iterations_options = [150, 175, 200, 225]\n",
    "depth_options = [1, 2, 3, 4]\n",
    "learning_rate_options = [.01, .025, 0.05, 0.1, .25]\n",
    "l2_reg_options = [3, 4, 5]  # L2 regularization values\n",
    "\n",
    "# Find the best hyperparameters\n",
    "for iterations in iterations_options:\n",
    "    for depth in depth_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            for l2_reg in l2_reg_options:\n",
    "                catboost = CatBoostClassifier(iterations=iterations, depth=depth, \n",
    "                                              learning_rate=learning_rate, \n",
    "                                              l2_leaf_reg=l2_reg,\n",
    "                                              cat_features=categorical_features_indices, \n",
    "                                              verbose=0)\n",
    "                pipeline = Pipeline([('classifier', catboost)])\n",
    "                \n",
    "                val_scores = []\n",
    "\n",
    "                for train_index, val_index in tscv.split(X_train):\n",
    "                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                    pipeline.fit(X_train_fold, y_train_fold)\n",
    "                    y_val_pred_proba = pipeline.predict_proba(X_val_fold)\n",
    "                    val_log_loss = log_loss(y_val_fold, y_val_pred_proba)\n",
    "                    val_scores.append(val_log_loss)\n",
    "\n",
    "                avg_val_log_loss = np.mean(val_scores)\n",
    "                if avg_val_log_loss < best_log_loss:\n",
    "                    best_log_loss = avg_val_log_loss\n",
    "                    best_params = (iterations, depth, learning_rate, l2_reg)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters: Iterations={best_params[0]}, Depth={best_params[1]}, LR={best_params[2]}, L2={best_params[3]}\")\n",
    "\n",
    "# (Continue with your model training and evaluation as needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a73db-0d1c-4940-8f32-47f26c0f0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assuming X_train, y_train, and categorical_features_indices are already defined\n",
    "\n",
    "# Define a TimeSeriesSplit cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "best_log_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "# Define your hyperparameters grid to be tested\n",
    "iterations_options = [150, 175, 200, 225]\n",
    "depth_options = [1, 2, 3, 4]\n",
    "learning_rate_options = [.01, .025, 0.05, 0.1, .25]\n",
    "l2_reg_options = [3, 4, 5]  # L2 regularization values\n",
    "\n",
    "# To track training and validation log losses\n",
    "all_train_log_losses = []\n",
    "all_val_log_losses = []\n",
    "\n",
    "# Find the best hyperparameters\n",
    "for iterations in iterations_options:\n",
    "    for depth in depth_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            for l2_reg in l2_reg_options:\n",
    "                catboost = CatBoostClassifier(iterations=iterations, depth=depth, \n",
    "                                              learning_rate=learning_rate, \n",
    "                                              l2_leaf_reg=l2_reg,\n",
    "                                              cat_features=categorical_features_indices, \n",
    "                                              verbose=0)\n",
    "                pipeline = Pipeline([('classifier', catboost)])\n",
    "                \n",
    "                train_log_losses = []\n",
    "                val_log_losses = []\n",
    "\n",
    "                for train_index, val_index in tscv.split(X_train, y_train):\n",
    "                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                    pipeline.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                    # Track training log loss\n",
    "                    y_train_pred_proba = pipeline.predict_proba(X_train_fold)\n",
    "                    train_log_loss = log_loss(y_train_fold, y_train_pred_proba)\n",
    "                    train_log_losses.append(train_log_loss)\n",
    "\n",
    "                    # Track validation log loss\n",
    "                    y_val_pred_proba = pipeline.predict_proba(X_val_fold)\n",
    "                    val_log_loss = log_loss(y_val_fold, y_val_pred_proba)\n",
    "                    val_log_losses.append(val_log_loss)\n",
    "\n",
    "                avg_train_log_loss = np.mean(train_log_losses)\n",
    "                avg_val_log_loss = np.mean(val_log_losses)\n",
    "\n",
    "                # Store the log losses for analysis\n",
    "                all_train_log_losses.append(train_log_losses)\n",
    "                all_val_log_losses.append(val_log_losses)\n",
    "\n",
    "                if avg_val_log_loss < best_log_loss:\n",
    "                    best_log_loss = avg_val_log_loss\n",
    "                    best_params = (iterations, depth, learning_rate, l2_reg)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters: Iterations={best_params[0]}, Depth={best_params[1]}, LR={best_params[2]}, L2={best_params[3]}\")\n",
    "\n",
    "# Analyze the stored log losses for overfitting\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78309316-aec6-4617-8b6b-be1fb3a65b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the training and validation log losses for the best hyperparameters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), train_log_losses, 'o-', color=\"r\", label=\"Training Log Loss\")\n",
    "plt.plot(range(1, 6), val_log_losses, 'o-', color=\"g\", label=\"Validation Log Loss\")\n",
    "plt.title('Training vs Validation Log Loss for Best Hyperparameters')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71bf69-02d8-43be-94a9-6be3be175e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_log_losses, label='Train Log Loss')\n",
    "plt.plot(val_log_losses, label='Validation Log Loss')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title(f'Training vs Validation Log Loss (Best Params: Iterations={best_params[0]}, Depth={best_params[1]}, LR={best_params[2]})')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107253cf-7865-4b93-abe0-195bf3b9d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have train_log_losses and val_log_losses from the previous cross-validation\n",
    "data_to_plot = [train_log_losses, val_log_losses]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data_to_plot, labels=['Train Log Loss', 'Validation Log Loss'])\n",
    "plt.title('Box Plot of Model Performance Across Folds')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374eeee3-f12c-451e-a050-26db1da1e740",
   "metadata": {},
   "source": [
    "Final Eval of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291515e4-ce3b-4f7c-9a69-0a8afb14d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best hyperparameters\n",
    "best_model = CatBoostClassifier(\n",
    "    iterations=75, \n",
    "    depth=2, \n",
    "    learning_rate=0.05, \n",
    "    l2_leaf_reg=1,\n",
    "    cat_features=categorical_features_indices, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "best_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940379d-d6d4-49a0-82f1-7deba5e4e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "test_log_loss = log_loss(y_test, y_pred_proba)\n",
    "print(\"Log Loss on Test Set:\", test_log_loss)\n",
    "\n",
    "# You may also compute other metrics, depending on your requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94cc52-07be-49b9-b5d5-37a1e66c28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050c4d7-7e5a-4859-8a4d-73c068ce8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming best_model is your trained CatBoostClassifier\n",
    "feature_importances = best_model.get_feature_importance()\n",
    "\n",
    "# Create a bar chart for feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.xticks(range(len(feature_importances)), X_train.columns, rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb0135-2a96-4cf0-8d7b-866f1b0a0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP explainer object using your trained CatBoost model\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_second_test)  # Replace X_second_test with your second test set features\n",
    "\n",
    "# For a specific instance (adjust instance_index as needed)\n",
    "instance_index = 0  # Change this index to explore different instances\n",
    "\n",
    "# Generate a force plot for that instance\n",
    "# Make sure to pass the correct arguments based on your SHAP version\n",
    "shap.force_plot(explainer.expected_value, shap_values[instance_index], X_second_test.iloc[instance_index], matplotlib=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a24af5-da9a-4eb6-8d97-e6246090fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the best_estimator from the GridSearchCV is already trained and available\n",
    "\n",
    "# Predicting probabilities for each class for each record in the test set\n",
    "class_probabilities_TS = best_model.predict_proba(X_test)\n",
    "\n",
    "# Displaying the first few records of the probabilities\n",
    "print(class_probabilities_TS[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f04191-08c5-437b-9e97-12b93dabb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the best_estimator from the GridSearchCV is already trained and available\n",
    "\n",
    "# Predicting probabilities for each class for each record in the test set\n",
    "class_probabilities = best_estimator.predict_proba(X_test)\n",
    "\n",
    "# Displaying the first few records of the probabilities\n",
    "print(class_probabilities[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745664a-19ca-4334-a406-45461a7aa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_df = pd.DataFrame(class_probabilities_TS, columns=['Probability_Home_win', 'Probability_Draw', 'Probability_Away_win'])\n",
    "\n",
    "# Create a new column in X_test to store the original index values\n",
    "X_test['original_index'] = X_test.index\n",
    "\n",
    "# Reset the index of X_test\n",
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "\n",
    "# Create the probability DataFrame\n",
    "probability_df_TS = pd.DataFrame(class_probabilities_TS, columns=['Probability_Home_win', 'Probability_Draw', 'Probability_Away_win'])\n",
    "\n",
    "# Concatenate the reset X_test DataFrame and the probability DataFrame\n",
    "result_df = pd.concat([X_test_reset, probability_df_TS], axis=1)\n",
    "\n",
    "# Displaying the first few records of the resulting DataFrame\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fa87e-039b-42ad-bac5-389cdd8106f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_prob = result_df[['original_index','Probability_Home_win', 'Probability_Draw', 'Probability_Away_win']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85850050-159d-4a2f-8458-b30ed0422249",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bce722-4b3d-45f9-9aae-0206eaab76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/lkimball/Desktop/Flatiron/CapstoneProject/test_predicted_prob.csv'\n",
    "test_predicted_prob.to_csv(csv_file_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e516c-e3e7-4da1-ad37-afc67d790281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('best_catboost_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dd0a2-4c9d-41eb-847c-912b625d80e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d57a69-8b7e-4d97-b29b-bea5bc083c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
