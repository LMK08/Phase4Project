{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b46044a-ff0c-4d82-aee2-71eb4c6ce04a",
   "metadata": {},
   "source": [
    "## Ensemble Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647cb92-8c37-4e27-88f5-8ae25d0f62b7",
   "metadata": {},
   "source": [
    "We are going to iterate through a collection of Machine Learning Classifiers including Logistic Regression, Random Forest, XGBoost, and CatBoost. We will also iterate through a series of hyperparameters for each model. Both of these steps are to find the optimimal performing model, based on F1 score.\n",
    "\n",
    "All of this modeling will be done using just the numeric features of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914928a3-6aac-4207-9847-4ec0112b4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187d9aee-3c90-40bb-9ace-891d285d8542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EPL_Updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc949cb-730c-4261-bad8-529c3f66b190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pinnacle Closing Home Win Odds', 'Pinnacle Closing Draw Odds',\n",
       "       'Pinnacle Closing Away Win Odds', 'date', 'home_team', 'away_team',\n",
       "       'week', 'date.1', 'home_team.1', 'home_xg', 'score', 'away_xg',\n",
       "       'away_team.1', 'referee', 'game_id', 'home_team_elo', 'away_team_elo',\n",
       "       'season', 'home_starters', 'away_starters', 'home_xG_to_date',\n",
       "       'away_xG_to_date', 'home_xG_against_to_date', 'away_xG_against_to_date',\n",
       "       'home_goals_scored', 'away_goals_scored', 'home_goals_scored_to_date',\n",
       "       'away_goals_scored_to_date', 'home_goals_conceded_to_date',\n",
       "       'away_goals_conceded_to_date', 'home_match_points', 'away_match_points',\n",
       "       'home_points_to_date', 'away_points_to_date', 'match_points',\n",
       "       'home_form', 'away_form', 'match_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851fa5da-7129-490a-955e-06e62e050586",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "All estimators failed to fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/jkmqh6017hb_3wwh71glzdrc0000gn/T/ipykernel_14757/1652977123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scaler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     grid_search_results.append({\n\u001b[1;32m     57\u001b[0m         \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# of out will be done in `_insert_error_scores`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                     \u001b[0m_insert_error_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_insert_error_scores\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccessful_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All estimators failed to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccessful_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: All estimators failed to fit"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('match_result', axis=1)\n",
    "y = df['match_result']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Defining the models and parameter grid\n",
    "model_params = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'LogisticRegression__C': [1e-6, 1e-4, 1e-2, 1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'RandomForestClassifier__n_estimators': [25, 50, 75],\n",
    "            'RandomForestClassifier__max_depth': [None, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        'params': {\n",
    "            'XGBoost__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "            'XGBoost__n_estimators': [100, 200, 300],\n",
    "            'XGBoost__max_depth': [3, 4, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'CatBoostClassifier': {\n",
    "    'model': CatBoostClassifier(verbose=0),  # Set verbose to 0 to prevent lots of output\n",
    "    'params': {\n",
    "        'CatBoostClassifier__depth': [4, 6, 8, 10],\n",
    "        'CatBoostClassifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "        'CatBoostClassifier__iterations': [100, 200, 300]\n",
    "    }\n",
    "}\n",
    "}\n",
    "\n",
    "# F1 score as the scoring metric\n",
    "# If 'match_result' is multi-class, use average='macro' to calculate scores for each label, and find their unweighted mean.\n",
    "# This does not take label imbalance into account.\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# List to store results of GridSearchCV\n",
    "grid_search_results = []\n",
    "\n",
    "# Loop through each model and perform grid search\n",
    "for model_name, mp in model_params.items():\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), (model_name, mp['model'])])\n",
    "    clf = GridSearchCV(pipe, mp['params'], cv=5, scoring=f1_scorer, return_train_score=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    grid_search_results.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_,\n",
    "        'best_estimator': clf.best_estimator_\n",
    "    })\n",
    "\n",
    "grid_search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc9d5c-6696-45a6-83e3-5c7c87d23c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from the hyperparameter tuning\n",
    "results = [\n",
    "    {'model': 'LogisticRegression', 'best_score': 0.399718514648683, 'best_params': {'LogisticRegression__C': 2}},\n",
    "    {'model': 'RandomForestClassifier', 'best_score': 0.42933520693584654, 'best_params': {'RandomForestClassifier__max_depth': None, 'RandomForestClassifier__n_estimators': 50}},\n",
    "    {'model': 'XGBoost', 'best_score': 0.44738501328112357, 'best_params': {'XGBoost__learning_rate': 0.5, 'XGBoost__max_depth': 5, 'XGBoost__n_estimators': 300}},\n",
    "    {'model': 'CatBoostClassifier', 'best_score': 0.45408248704088583, 'best_params': {'CatBoostClassifier__depth': 10, 'CatBoostClassifier__iterations': 300, 'CatBoostClassifier__learning_rate': 0.1}}\n",
    "]\n",
    "\n",
    "# Extract model names and their best scores\n",
    "model_names = [r['model'] for r in results]\n",
    "best_scores = [r['best_score'] for r in results]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(model_names, best_scores, color='skyblue')\n",
    "plt.xlabel('Best F1 Score')\n",
    "plt.title('Best F1 Score for Different Models')\n",
    "plt.xlim(0, 0.5)  # Assuming the F1 scores are between 0 and 0.5\n",
    "for index, value in enumerate(best_scores):\n",
    "    plt.text(value, index, str(round(value, 4)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06742f92-78ba-45eb-9a77-8bb65ae89460",
   "metadata": {},
   "source": [
    "Above we can see the performance of the optimized models, with CatBoost being the top performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c7c89-9621-4569-9831-10b0c303c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'grid_search_results' is a list of dictionaries with your grid search results\n",
    "# which includes the best estimator for each model\n",
    "\n",
    "# Feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Initialize a figure\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Loop through each model's best estimator and plot feature importances\n",
    "for idx, result in enumerate(grid_search_results):\n",
    "    model_name = result['model']\n",
    "    best_estimator = result['best_estimator']\n",
    "    \n",
    "    # Get feature importances\n",
    "    if model_name in ['RandomForestClassifier', 'XGBoost']:\n",
    "        importances = best_estimator.named_steps[model_name].feature_importances_\n",
    "    elif model_name == 'LogisticRegression':\n",
    "        # For LogisticRegression, take the absolute value of coefficients as importance\n",
    "        importances = np.abs(best_estimator.named_steps[model_name].coef_[0])\n",
    "    elif model_name == 'CatBoostClassifier':\n",
    "        # For CatBoost, use the get_feature_importance method\n",
    "        importances = best_estimator.named_steps[model_name].get_feature_importance()\n",
    "    \n",
    "    # Sort the feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the feature importances\n",
    "    ax = axes[idx]\n",
    "    ax.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "    ax.set_xticks(range(len(importances)))\n",
    "    ax.set_xticklabels(feature_names[indices], rotation=90)\n",
    "    ax.set_xlim([-1, len(importances)])\n",
    "    ax.set_title(f'Feature Importances for {model_name}')\n",
    "    ax.set_ylabel('Importance')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364c4af-a973-4697-b47d-e7880a4a98d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
